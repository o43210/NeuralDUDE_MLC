{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import binary_dude as bd\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.utils.training_utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "K.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.025 1.25  0.   ]\n",
      " [1.025 0.    1.25 ]]\n",
      "(65536,)\n",
      "[0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# n=1000000\n",
    "alpha=0.1\n",
    "delta=0.1\n",
    "nb_classes=2\n",
    "L=np.array([[delta, -delta/(1-2*delta), (1-delta)/(1-2*delta)],[delta, (1-delta)/(1-2*delta), -delta/(1-2*delta)]])\n",
    "L_new=-L+(1-delta)/(1-2*delta)     # A new loss matrix\n",
    "k_max=40\n",
    "version = int(sys.argv[4])\n",
    "\n",
    "print L_new\n",
    "\n",
    "n = 65536\n",
    "x = bd.bsmc(n, alpha)\n",
    "z = bd.bsc(x,delta)\n",
    "Z=utils.np_utils.to_categorical(z,nb_classes)\n",
    "\n",
    "print x.shape\n",
    "print z[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DUDE with EM Algorithm(v1)\n",
    "* 아래 코드는 iteration 20번을 모두 돌린 후 얻은 delta로 noise 를 제거한 x_hat에 대한 error rate을 출력\n",
    "\n",
    "ipython notebook으로 돌릴 경우\n",
    "\n",
    "    #k1 = int(sys.argv[1])\n",
    "    #k2 = int(sys.argv[2])\n",
    "    #gpu_num = int(sys.argv[3])\n",
    "    \n",
    "    k1 = ?\n",
    "    k2 = ?\n",
    "    gpu_num = ?\n",
    "    \n",
    "위 경우처럼 돌려야 함.\n",
    "\n",
    "python file 로 돌릴경우\n",
    "\n",
    "    k1 = int(sys.argv[1])\n",
    "    k2 = int(sys.argv[2])\n",
    "    gpu_num = int(sys.argv[3])\n",
    "    \n",
    "    #k1 = ?\n",
    "    #k2 = ?\n",
    "    #gpu_num = ?\n",
    "    \n",
    "python file 실행할 때 argument로 k1, k2, gpu_num을 설정해 주어야함. 아래와 같이 screen 에서 명령어 입력.\n",
    "\n",
    "    \"\"CUDA_VISIBLE_DEVICES=? k1 k2 ? version#\"\"\n",
    "    CUDE_VISIBLE_DEVICES=0 1 4 0 1 -> 이렇게 돌리면 됨. \n",
    "    위와 같이 돌리면 window size = 1~4로 GPU0에 돌리고 version = 1 이다.\n",
    "    \n",
    "    CUDE_VISIBLE_DEVICES=0 1 4 0 1\n",
    "    CUDE_VISIBLE_DEVICES=1 5 8 1 1\n",
    "    CUDE_VISIBLE_DEVICES=2 9 12 2 1\n",
    "    CUDE_VISIBLE_DEVICES=3 13 15 3 1\n",
    "    -> 이렇게 4개의 GPU에 4개의 코드를 돌리면 효율적임.\n",
    "    \n",
    "* 이렇게 설정한 이유는 여러 GPU에 서로 다른 코드를 돌리기 위함임.\n",
    "* 그리고 결과는 text file로 저장하세요... 아래와 같이 저장하면 됩니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1\n",
      "delta: 0.300000\n",
      "delta: 0.157103\n"
     ]
    }
   ],
   "source": [
    "if version == 1:\n",
    "    \n",
    "    delta = None\n",
    "\n",
    "    k1 = int(sys.argv[1])\n",
    "    k2 = int(sys.argv[2])\n",
    "    gpu_num = int(sys.argv[3])\n",
    "\n",
    "    #k1 = 1\n",
    "    #k2 = 4\n",
    "    #gpu_num = 0\n",
    "\n",
    "    f = open(\"NeuralDUDE_EM_v1_result_gpu%d.txt\"%gpu_num,\"w\")\n",
    "    f.write(\"gpu%d result\\n\"%gpu_num)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    for k in xrange(k1,k2+1):\n",
    "        print 'k=',k\n",
    "        with open(\"NeuralDUDE_EM_v1_result_gpu%d.txt\"%gpu_num,\"a\") as f:\n",
    "            f.write('k=%d\\n'%k)\n",
    "\n",
    "        b = np.array([[0.7, 0.3],[0.3, 0.7]])\n",
    "        model = None\n",
    "\n",
    "        for t in xrange(20):\n",
    "            # -----------------------------------------------------\n",
    "            # For directly generating data for Neural DUDE\n",
    "            # -----------------------------------------------------\n",
    "            delta = b[0][1]\n",
    "            L=np.array([[delta, -delta/(1-2*delta), (1-delta)/(1-2*delta)],[delta, (1-delta)/(1-2*delta), -delta/(1-2*delta)]])\n",
    "            L_new=-L+(1-delta)/(1-2*delta)     # A new loss matrix\n",
    "            C,Y = bd.make_data_for_ndude(Z,k,L_new,nb_classes,n)\n",
    "            #print C.shape\n",
    "            #print Y.shape\n",
    "            gamma = np.zeros((C.shape[0], nb_classes))\n",
    "            print 'delta: %f'%delta\n",
    "\n",
    "            # -----------------------------------------------------\n",
    "            # Defining neural networks\n",
    "            # -----------------------------------------------------\n",
    "\n",
    "            inputs = layers.Input(shape = (2 * k * nb_classes,))\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(inputs)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(3, kernel_initializer = 'he_normal')(layer)\n",
    "            outputs = layers.Activation('softmax')(layer)\n",
    "            model = models.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "            adam = optimizers.Adam()\n",
    "            model.compile(loss='poisson', optimizer=adam)\n",
    "\n",
    "            #print 'Model fitting...'\n",
    "            model.fit(C,Y,epochs=10,batch_size=100, verbose=0, validation_data=(C, Y))\n",
    "            # -----------------------------------------------------\n",
    "\n",
    "            pred_prob = model.predict(C, batch_size = 200, verbose = 0)\n",
    "            gamma[:,0] = pred_prob[:, 1]\n",
    "            gamma[:,1] = pred_prob[:, 2]\n",
    "            gamma[np.arange(C.shape[0]), z[k+1:n-k+1]] += pred_prob[np.arange(C.shape[0]), 0]\n",
    "\n",
    "            b_before = b\n",
    "            b = np.zeros((nb_classes, nb_classes))\n",
    "            np.add.at(b.T, z[k+1:n-k+1], gamma)\n",
    "            b /= (np.sum(gamma, axis = 0).reshape(nb_classes,1) + 1e-35)\n",
    "\n",
    "            if rel_error(b, b_before) < 1e-6:\n",
    "                model_ans = model\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        pred_prob = model.predict(C, batch_size=200, verbose=0)\n",
    "        pred_class = np.argmax(pred_prob, axis = 1)\n",
    "        s_nn_hat=hstack((zeros(k),pred_class,zeros(k)))\n",
    "        x_nn_hat=bd.denoise_with_s(z,s_nn_hat,k)\n",
    "        error_nn=bd.error_rate(x,x_nn_hat)\n",
    "        print 'error_nn=', error_nn\n",
    "        print 'pred_delta: %f'%delta\n",
    "\n",
    "        with open(\"NeuralDUDE_EM_v1_result_gpu%d.txt\"%gpu_num,\"a\") as f:\n",
    "            f.write('error_nn = %f\\n'%error_nn)\n",
    "            f.write('pred_delta =  %f\\n'%delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DUDE with EM Algorithm(v2)\n",
    "* 아래 코드는 iteration 20번 중 error rate 이 가장 작은 error을 출력\n",
    "* v1과 다르게 iteration을 수행하면서 noise를 제거하기 때문에 더 느리다.\n",
    "* 실행 방법은 v1과 똑같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if version == 2:\n",
    "\n",
    "    k1 = int(sys.argv[1])\n",
    "    k2 = int(sys.argv[2])\n",
    "    gpu_num = int(sys.argv[3])\n",
    "\n",
    "    #k1 = 1\n",
    "    #k2 = 4\n",
    "    #gpu_num = 0\n",
    "\n",
    "    f = open(\"NeuralDUDE_EM_v2_result_gpu%d.txt\"%gpu_num,\"w\")\n",
    "    f.write(\"gpu%d result\\n\"%gpu_num)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    for k in xrange(k1,k2+1):\n",
    "        print 'k=',k\n",
    "        with open(\"NeuralDUDE_EM_v2_result_gpu%d.txt\"%gpu_num,\"a\") as f:\n",
    "            f.write('k=%d\\n'%k)\n",
    "\n",
    "        b = np.array([[0.7, 0.3],[0.3, 0.7]])\n",
    "\n",
    "        opt_delta = None\n",
    "        error_nn = 1\n",
    "        opt_t = 0\n",
    "\n",
    "        for t in xrange(20):\n",
    "            # -----------------------------------------------------\n",
    "            # For directly generating data for Neural DUDE\n",
    "            # -----------------------------------------------------\n",
    "            delta = b[0][1]\n",
    "            L=np.array([[delta, -delta/(1-2*delta), (1-delta)/(1-2*delta)],[delta, (1-delta)/(1-2*delta), -delta/(1-2*delta)]])\n",
    "            L_new=-L+(1-delta)/(1-2*delta)     # A new loss matrix\n",
    "            C,Y = bd.make_data_for_ndude(Z,k,L_new,nb_classes,n)\n",
    "            #print C.shape\n",
    "            #print Y.shape\n",
    "            gamma = np.zeros((C.shape[0], nb_classes))\n",
    "            print 'delta: %f'%delta\n",
    "\n",
    "            # -----------------------------------------------------\n",
    "            # Defining neural networks\n",
    "            # -----------------------------------------------------\n",
    "\n",
    "            inputs = layers.Input(shape = (2 * k * nb_classes,))\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(inputs)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(40, kernel_initializer = 'he_normal')(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            layer = layers.Dense(3, kernel_initializer = 'he_normal')(layer)\n",
    "            outputs = layers.Activation('softmax')(layer)\n",
    "            model = models.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "            adam = optimizers.Adam()\n",
    "            model.compile(loss='poisson', optimizer=adam)\n",
    "\n",
    "            #print 'Model fitting...'\n",
    "            model.fit(C,Y,epochs=10,batch_size=100, verbose=0, validation_data=(C, Y))\n",
    "            # -----------------------------------------------------\n",
    "\n",
    "            pred_prob = model.predict(C, batch_size = 200, verbose = 0)\n",
    "            pred_class = np.argmax(pred_prob, axis = 1)\n",
    "            s_nn_hat=hstack((zeros(k),pred_class,zeros(k)))\n",
    "            x_nn_hat=bd.denoise_with_s(z,s_nn_hat,k)\n",
    "            error = bd.error_rate(x, x_nn_hat)\n",
    "            print 'error: %f'%error\n",
    "            if error_nn > error:\n",
    "                error_nn = error\n",
    "                opt_delta = delta\n",
    "                opt_t = t\n",
    "\n",
    "            gamma[:,0] = pred_prob[:, 1]\n",
    "            gamma[:,1] = pred_prob[:, 2]\n",
    "            gamma[np.arange(C.shape[0]), z[k+1:n-k+1]] += pred_prob[np.arange(C.shape[0]), 0]\n",
    "\n",
    "            b_before = b\n",
    "            b = np.zeros((nb_classes, nb_classes))\n",
    "            np.add.at(b.T, z[k+1:n-k+1], gamma)\n",
    "            b /= (np.sum(gamma, axis = 0).reshape(nb_classes,1) + 1e-35)\n",
    "\n",
    "        print('error_nn = %f\\n'%error_nn)\n",
    "        print('pred_delta =  %f\\n'%opt_delta)\n",
    "        print('iteration: %d\\n'%opt_t) \n",
    "\n",
    "        with open(\"NeuralDUDE_EM_v2_result_gpu%d.txt\"%gpu_num,\"a\") as f:\n",
    "            f.write('error_nn = %f\\n'%error_nn)\n",
    "            f.write('pred_delta =  %f\\n'%opt_delta)\n",
    "            f.write('iteration: %d\\n'%opt_t) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
